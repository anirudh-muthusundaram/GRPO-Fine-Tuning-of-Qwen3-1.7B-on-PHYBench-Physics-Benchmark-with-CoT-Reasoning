# ğŸ”¬ GRPO Fine-Tuning of Qwen3-1.7B on PHYBench Physics Benchmark with CoT Reasoning

Fineâ€‘tuning **Qwenâ€¯3â€‘1.7â€¯B** with **Groupâ€¯Relativeâ€¯Policyâ€¯Optimization (GRPO)** and **Chainâ€‘ofâ€‘Thought (CoT)** prompting on the openâ€‘source **PHYBench** physics reasoning benchmark.

---

## âœ¨ Project Highlights

* **Physicsâ€‘specific reward shaping** â€“ composite reward balances boxedâ€‘answer formatting, reasoning trace length, and symbolic correctness.
* **CoT prompting** â€“ the model is instructed to *think stepâ€‘byâ€‘step* and enclose the final answer in `\boxed{â€¦}`.
* **LoRAâ€‘64 / 4â€‘bit QLoRA** â€“ memoryâ€‘efficient adaptation with **Unsloth**â€‘style 4â€‘bit quantisation, later merged back to full FP32.
* **Fullâ€‘logging GRPO** â€“ logs KLâ€‘divergence, reward components, learningâ€‘rate schedule, and completion lengths at every step via a custom `ConsoleLoggerCallback`.
* **Oneâ€‘command FP32 export** â€“ script automatically merges adapters and saves a readyâ€‘toâ€‘push FP32 checkpoint.

---

## ğŸ”§ Quickâ€‘start

```bash
# 1. Create and activate environment (PyTorch â‰¥ 2.1, CUDA 11.8)
conda create -n qwen-grpo python=3.10 -y && conda activate qwen-grpo

# 2. Install requirements
pip install -r requirements.txt  # transformers, trl >=0.17, peft, datasets, sympy, accelerate, bitsandbytes, unsloth, etc.

# 3. Launch training (default hyperâ€‘params)
python phybench_grpo_qwen3.py \
  --model_name Qwen/Qwen3-1.7B \
  --dataset_name Eureka-Lab/PHYBench \
  --split_full PHYBench-fullques_v1.json \
  --output_dir runs/qwen3_phybench_grpo
```

Training logs look like:

```
[Step 410] reward/phybench_reward=0.69 kl/mean=0.03 lr=4.7eâ€‘7 len/avg_comp=512 â€¦
```

---

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ phybench_grpo_qwen3.py   # main training script (this repo)
â”œâ”€â”€ requirements.txt         # python dependencies
â”œâ”€â”€ runs/                    # output checkpoints & tensorboard logs
â”‚   â””â”€â”€ qwen3_phybench_grpo/
â”‚       â”œâ”€â”€ adapter_model/   # LoRA adapters (during training)
â”‚       â””â”€â”€ fp32/            # merged fullâ€‘precision model + tokenizer
â””â”€â”€ README.md
```

---

## âš™ï¸ Key Hyperâ€‘parameters

| Parameter         | Value | Notes                            |
| ----------------- | ----- | -------------------------------- |
| `max_prompt_len`  | 4096  | accommodates CoT traces          |
| `max_new_tokens`  | 1024  | completion budget                |
| `num_generations` | 4     | parallel GRPO rollouts           |
| `beta` (KL)       | 0.1   | stabilises divergence            |
| `learning_rate`   | 5eâ€‘7  | tuned for 4â€‘bit QLoRA            |
| `warmup_ratio`    | 0.03  | linear LR schedule               |
| `r` (LoRA rank)   | 64    | target modules `q_proj`,`v_proj` |

Reward weights:

```python
w_format  = 0.2   # presence of \boxed{â€¦}
w_trace   = 0.3   # â‰¥6 newlineâ€‘separated reasoning steps
w_correct = 0.5   # EEDâ€‘style symbolic equivalence
```

---

## ğŸ“Š Results

| Checkpoint        | Mean Total Reward | Format (âœ“) | TraceÂ â‰¥â€¯6 | EED Correct | Notes                      |
| ----------------- | ----------------- | ---------- | --------- | ----------- | -------------------------- |
| base (zeroâ€‘shot)  | 0.19              | 43â€¯%       | 22â€¯%      | 11â€¯%        | Qwenâ€¯3â€‘1.7â€¯B, no fineâ€‘tune |
| **GRPOâ€‘PHYBench** | **0.54**          | **76â€¯%**   | **68â€¯%**  | **38â€¯%**    | 550 steps, 2Ã—2 batch       |

> *EED correct* uses an expressionâ€‘equivalence distance (`sympy.simplify`) plus sizeâ€‘normalised tolerance.

---

## ğŸ“ Dataset â€“ PHYBench

* Source: **Eurekaâ€‘Lab/PHYBench** (HuggingFace Datasets)
* 1â€¯699 AP + IIT style multipleâ€‘paragraph physics questions with worked solutions.
* Preâ€‘processing extracts:

  * `prompt` â€“ chat template with system + user roles
  * `answer` â€“ last `\boxed{â€¦}` expression in the published solution

---

## ğŸ” Evaluation

A minimal evaluation harness is provided in `eval_phybench.py` (WIP):

```bash
python eval_phybench.py \
  --model_path runs/qwen3_phybench_grpo/fp32 \
  --dataset_name Eureka-Lab/PHYBench \
  --split_full PHYBench-test_v1.json
```

Metrics reported: *Total reward*, *Exact EED*, *Format accuracy*, *Traceâ€‘length accuracy*.

---

## ğŸ“¤ Pushing to the Hub

After training, the script outputs a merged FP32 folder:

```bash
runs/qwen3_phybench_grpo/fp32/
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ modelâ€‘00001â€‘ofâ€‘00003.safetensors
â”œâ”€â”€ pytorch_model.bin.index.json
â””â”€â”€ tokenizer.json
```

Publish with:

```bash
huggingface-cli repo create qwen3-grpo-phybench
HF_HOME=~/.cache/huggingface \
python -m transformers.models.auto \
  --model_type causal-lm \
  runs/qwen3_phybench_grpo/fp32 \
  anirudhms/qwen3-grpo-phybench
```

---

## ğŸ–‡ï¸ Citation

If you use this repository, please cite:

```bibtex
@misc{muthusundaram2025qwengrpo,
  title   = {Chainâ€‘ofâ€‘Thought Fineâ€‘Tuning of QwenÂ 3â€‘1.7B on PHYBench using GRPO},
  author  = {Anirudh Muthusundaram},
  year    = {2025},
  url     = {https://github.com/yourâ€‘username/qwenâ€‘grpoâ€‘phybench}
}
```

---

## ğŸ“œ License

This project is licensed under the **ApacheÂ 2.0** license. See the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgements

* **Alibaba** for the Qwen model series.
* **TRL / HuggingFace** for the GRPO implementation.
* **Unsloth** for the 4â€‘bit adapter merge utilities.
* **Eurekaâ€‘Lab** for releasing the PHYBench dataset.

---
